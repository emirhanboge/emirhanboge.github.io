<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://emirhanboge.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emirhanboge.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-06T22:41:45+00:00</updated><id>https://emirhanboge.github.io/feed.xml</id><title type="html">Emirhan Böge</title><subtitle></subtitle><entry><title type="html">What does BERT look at? An Analysis of BERT’s Attention (ACL 2019)</title><link href="https://emirhanboge.github.io/posts/2024/first-post/" rel="alternate" type="text/html" title="What does BERT look at? An Analysis of BERT’s Attention (ACL 2019)"/><published>2024-11-06T00:00:00+00:00</published><updated>2024-11-06T00:00:00+00:00</updated><id>https://emirhanboge.github.io/posts/2024/first-post</id><content type="html" xml:base="https://emirhanboge.github.io/posts/2024/first-post/"><![CDATA[<p>I do 3 passes of reading a paper. The first pass is to get a general idea of what the paper is about. The second pass is to understand the details of the paper. The third pass is to understand the paper in depth.</p> <p>For this particular paper I did {FILL IN} passes of reading.</p> <h1 id="first-pass">First Pass</h1> <ol> <li><strong>What is the problem?</strong> The paper investigates what BERT attends to when making predictions.</li> <li><strong>Why is it important?</strong> Understanding what BERT or any other language model attends to is important as it can help us understand how these models <strong>reason</strong>.</li> </ol> <h3 id="abstract">Abstract</h3> <p>Previous work focused on:</p> <ul> <li>Model outputs (final layer of the model)</li> <li>Internal vector representations (this could be the hidden states of the model)</li> </ul> <p>BERT’s attention heads focused on:</p> <ul> <li>Delimeter tokens (e.g. [CLS], [SEP])</li> <li>Some positional offsets, meaning that the attention heads are not just focusing on the token itself but also on the tokens around it.</li> <li>Or broadly on the entire sentence.</li> </ul> <p>Some attention heads reflected already known linguistic phenomena:</p> <ul> <li>e.g. attention heads that focused on objects of verbs, coreference, etc.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>The paper proposed different methods to understand attention.</p> <p>They state that in the attention maps linguistic knowledge can be found. They do something called <strong>probing attention maps</strong> to understand what the attention heads are focusing on.</p> <p>So I believe the paper shown that BERT reflects linguistic phenomena in its attention heads. This is a very revealing insight into how BERT works.</p> <h3 id="introduction">Introduction</h3> <p>Heads learn different types of information, in this case linguistic information. Language models like BERT by training on unlabeled data inherently learns linguistic information, thus there is no need for explicit linguistic features.</p> <h1 id="second-pass">Second Pass</h1>]]></content><author><name></name></author><category term="research-notes"/><category term="research-notes"/><summary type="html"><![CDATA[My notes on the paper.]]></summary></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://emirhanboge.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emirhanboge.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-13T08:36:47+00:00</updated><id>https://emirhanboge.github.io/feed.xml</id><title type="html">Emirhan Böge</title><subtitle></subtitle><entry><title type="html">Notes on the paper - Towards Modular LLMs by Building and Reusing a Library of LoRAs</title><link href="https://emirhanboge.github.io/posts/2024/11/13/towards-modular-llms/" rel="alternate" type="text/html" title="Notes on the paper - Towards Modular LLMs by Building and Reusing a Library of LoRAs"/><published>2024-11-13T00:00:00+00:00</published><updated>2024-11-13T00:00:00+00:00</updated><id>https://emirhanboge.github.io/posts/2024/11/13/towards-modular-llms</id><content type="html" xml:base="https://emirhanboge.github.io/posts/2024/11/13/towards-modular-llms/"><![CDATA[<p>My notes on the paper by Oleksiy Ostapenko, Zhan Su, Edoardo Maria Ponti, Laurent Charlin, Nicolas Le Roux, Matheus Pereira, Lucas Caccia, and Alessandro Sordoni. The paper was presented at NeurIPS 2024. The paper can be found <a href="https://arxiv.org/pdf/2405.11157">here</a>.</p> <h1 id="first-pass">First Pass</h1> <p>Parameter-efficient adaptations of LLMs are significant =&gt; Can we reuse trained adapters across tasks?</p> <p>Model-based clustering (MBC) =&gt; Group tasks based on the similarity of their adapters. Find similarity (positive correlation) between LoRA weights of two tasks, and then transfer between those tasks.</p> <p>Arrow =&gt; Dynamic selection of relevant adapters for a new task withoout retraining.</p> <p>Previously few-shot adaptation with pretrained adapters was researched. This paper extends this idea to zero-shot adaptation.</p> <p>For each task train an adapter is trained on multi-task data, then these adapters are mixed for unseen tasks.</p> <h1 id="second-pass">Second Pass</h1> <p>Private Adapters =&gt; For each task train a adapter.</p> <p>Shared Adapter =&gt; Train a single adapter on all tasks. Might be problematic since the model cannot fit all tasks, (solution: increase the number of trainable parameters).</p> <p>Poly/MHR Adapters =&gt; Combination of private and shared adapters.</p> <p><strong>Model-Based Clustering (MBC)</strong> =&gt; Create clusters of tasks and train one adapter per task cluster. 1. Train private LoRAs, 2. Group LoRA parameters into K clusters using K-means, 3. Train one adapter per cluster.</p> <hr/> <p>Mu Routing =&gt; LoRA adapters are linear, thus routing to exxisting experts could be done using uniform distribution. (the simplest method, average the weights of the adapters uniformly).</p> <p>Arrow Routing =&gt; Estimating routing matrix by SVD(AB^T) and taking the right first singular vector, and compute the direction of most variance.</p> <p>The paper used LoRA rank of 4, dropout of 0.05, learning rate of 1e-4, and MBC number of clusters of 10.</p> <p>MBC and Arrow enhances the performance compared to Full Fine-Tuning.</p> <h1 id="third-pass">Third Pass</h1> <ul> <li>Can we improve upon K-means clustering, and find the number of clusters automatically? Maybe using the semantics of the tasks (e.g. using the task descriptions).</li> <li>Can we also share parameters across clusters?</li> </ul>]]></content><author><name></name></author><category term="lit-notes"/><category term="lora"/><category term="llm"/><category term="modular"/><summary type="html"><![CDATA[My notes on the paper by Oleksiy Ostapenko, Zhan Su, Edoardo Maria Ponti, Laurent Charlin, Nicolas Le Roux, Matheus Pereira, Lucas Caccia, and Alessandro Sordoni. The paper was presented at NeurIPS 2024. The paper can be found here.]]></summary></entry><entry><title type="html">Notes on the paper - LoRA:Low-Rank Adaptation of Large Language Models (ICLR 2022)</title><link href="https://emirhanboge.github.io/posts/2024/11/10/lora/" rel="alternate" type="text/html" title="Notes on the paper - LoRA:Low-Rank Adaptation of Large Language Models (ICLR 2022)"/><published>2024-11-10T00:00:00+00:00</published><updated>2024-11-10T00:00:00+00:00</updated><id>https://emirhanboge.github.io/posts/2024/11/10/lora</id><content type="html" xml:base="https://emirhanboge.github.io/posts/2024/11/10/lora/"><![CDATA[<p>My notes on the paper by Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. The paper was presented at ICLR 2022. The paper can be found <a href="https://arxiv.org/pdf/2106.09685">here</a>.</p> <h1 id="first-pass">First Pass</h1> <ol> <li><strong>What is the problem?</strong> The paper investigates how to adapt large language models to new tasks with fewer parameters.</li> <li><strong>Why is it important?</strong> Adapting large language models to new tasks with fewer parameters can reduce the computational cost and memory footprint of these models.</li> </ol> <p>LLMs fine-tuning to new tasks is computationall expensive as they have lots of parameters, and all of them are trainable. The paper proposes a method that freezes all the original parameters and <strong>injects</strong> new trainable rank decomposition matrices. The parameters are reduced 10.000x compared to the original model. The method is called LoRA.</p> <p>The previous methods had a problem in inference latency, which is the time it takes to make a prediction, and they reduce the model’s usable sequence length, which is the length of the input sequence the model can handle. Moreover, they also does not match the baselines. The paper proposes a method that does not have these problems.</p> <p>Li et al. (2018); Aghajanyan et al. (2020) =&gt; learned over-parametrized models depend on low intrinsic dimension.</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes. arXiv:1804.08838 [cs, stat], April 2018a. URL http: //arxiv.org/abs/1804.08838. arXiv: 1804.08838.</span>

<span class="c">Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], December 2020. URL http://arxiv.org/abs/2012.13255.</span>
</code></pre></div></div> <ul> <li>We can use many matrices for many tasks, freeze one and switch to the other matrix to work on another task.</li> <li>Only injected smaller low-rank matrices are optimized.</li> </ul> <h1 id="second-pass">Second Pass</h1> <p>For each different task traditionally we learn a different set of parameters, so having many independent fine-tuned models is not feasible. Lora hence is a suitable method for adapting to many new tasks with fewer parameters.</p> <p>Aghajanyan et al. (2020) =&gt; random projection to a smaller subspace can still work.</p> <p>LoRA =&gt; Weights also have a low intrinsic rank.</p> <p>Original weights are frozen, matrices A and B are injected and optimized.</p> \[W = W_0 + B \cdot A\] <p>A =&gt; Random Gaussian initialization</p> <p>B =&gt; Zero initialization</p> <p>LoRA generalizes to original fine-tuning when the number of parameters increases to the number of original parameters.</p> <p>Note: in this study only the attention weights are adapted, no MLPs.</p> <ul> <li>They compared it to BitFit: only bias vectors are trained, rest is frozen. (Zaken et al., 2021)</li> </ul> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models, 2021.</span>
</code></pre></div></div> <h1 id="third-pass">Third Pass</h1> <ul> <li>Can we represent A and B in a more compact way?</li> <li>Can we use a different initialization for A and B?</li> <li>Can we use a different optimization strategy for A and B? (e.g. Adam for A and SGD for B). This is because B is initialized to zero and Adam might not be able to update it properly.</li> </ul> <h4 id="references">References</h4> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">hu2021lora</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Lora: Low-rank adaptation of large language models}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2106.09685}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2021}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="lit-notes"/><category term="parameter-efficient"/><category term="transformers"/><category term="nlp"/><category term="llm"/><summary type="html"><![CDATA[My notes on the paper by Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. The paper was presented at ICLR 2022. The paper can be found here.]]></summary></entry><entry><title type="html">Notes on the paper - LLM.int8() 8-bit Matrix Multiplication for Transformers at Scale (NeurIPS 2022)</title><link href="https://emirhanboge.github.io/posts/2024/11/07/llm-8int/" rel="alternate" type="text/html" title="Notes on the paper - LLM.int8() 8-bit Matrix Multiplication for Transformers at Scale (NeurIPS 2022)"/><published>2024-11-07T00:00:00+00:00</published><updated>2024-11-07T00:00:00+00:00</updated><id>https://emirhanboge.github.io/posts/2024/11/07/llm-8int</id><content type="html" xml:base="https://emirhanboge.github.io/posts/2024/11/07/llm-8int/"><![CDATA[<p>My notes on the paper by Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. The paper was presented at NeurIPS 2022. The paper can be found <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf">here</a>.</p> <h1 id="first-pass">First Pass</h1> <ol> <li><strong>What is the problem?</strong> The paper investigates how to quantize the attention mechanism in transformers to 8-bit precision.</li> <li><strong>Why is it important?</strong> Quantizing the attention mechanism in transformers to 8-bit precision can reduce the memory footprint and computational cost of transformers.</li> </ol> <h3 id="abstract">Abstract</h3> <p>An llm quantization method with in8 matrix multiplication. Memory required for inference is reduced by half. The most interesting part for me is that the authors state that there are <strong>emergent features</strong> in transformers. They use vector-wise quantization to quantize the attention mechanism in transformers. Most importantly, for the emergent features they use a different way of decomposing the attention mechanism.</p> <h3 id="introduction">Introduction</h3> <p>The feed-forward and attention layers with their matrix multiplication operations make up the 95% of the parameters and 65-85% of the computation for large transformer language models (For LLMs with &gt;= 6.7B parameters) (Ilharco et al., 2020).</p> <p>First multi-billion scale Int8 quantization procedure.</p> <ul> <li>32-bit weights to 8-bit weights</li> </ul> <p>With vector-wise quantization the method retains performance up to 2.7B parameters. Beyond 6.7B parameters emergence of extreme outliers in the feature dimensions during inference, making up 0.1% of all input features. Zeroing these features out decreases top-1 attention softmax probability mass by more than 20%, and affects validation perplexity by 600-1000%.</p> <p>To compensate for these outliers the paper performs 16-bit matrix multiplication on the outliers, and vector-wise quantization on the rest of the features (99.9%).</p> <h3 id="broader-impacts">Broader Impacts</h3> <p>Making LLMs more accessible for future research. This could lead to both positive and negative impacts.</p> <h1 id="second-pass">Second Pass</h1> <h3 id="background">Background</h3> <p>High-precision asymmetric quantization (zeropoint quantization): high precision by using the full bit-range but is rarely used in practice due to the high memory and computational cost. Scaling with the normalized dynamic range and then shifting by the zeropoint.</p> <p>Symmetric quantization (absolute maximum quantization): the most commonly used technique. Dividing by the infinity norm and multiplying by 127. Values in [-127, 0) are not used.</p> <h3 id="int8-matrix-multiplicatiobn-at-scale-method-of-the-paper">Int8 Matrix Multiplicatiobn at Scale (Method of the paper)</h3> <p>Main challenge: quantization methods use a single scaling constant per tensor, an outlier could reduce precision.</p> <h4 id="vector-wise-quantization">Vector-wise Quantization</h4> <p>View matrix multiplication as a sequence of independent inner products.</p> <p>Hidden States: $X_{f16} \in \mathbb{R}^{b \times h}$</p> <p>Weight Matrix: $W_{f16} \in \mathbb{R}^{h \times o}$</p> <p>Assign scaling constant $c_{x_{f16}}$ to each row of $X_{f16}$ and $c_w$ to each row of $W_{f16}$.</p> <p>Dequantize by: $\frac{1}{c_{x_{f16}}c_{w_{f16}}}$</p> <h4 id="mixed-precision-decomposition-for-emergent-features">Mixed-precision Decomposition (for emergent features)</h4> <p>For the emergent features, the authors use a different way of decomposing the attention mechanism. These are important for transformer performance, and thus they need higher precision. As they are scarce in the input, they can be processed with higher precision and without a significant increase in memory or computational cost.</p> <p>Outliers should have a magnitude larget than the threshold $\alpha$. This paper uses $\alpha = 6.0$.</p> <p>For evaluation:</p> <ul> <li>Perplexity (authors say this is a robust measure sensitive to quantization)</li> <li>Zeroshot accuracy degradation</li> </ul> <p><strong>Results</strong>: absmax, row-wise, and zeropoint quantization methods fail as the parameters scale up, wheras LLM.int8() retains performance. It runs two times faster for large matrix multiplications (up to 175B parameters).</p> <h3 id="emergent-large-magnitude-features-in-transformers-at-scale">Emergent Large Magnitude Features in Transformers at Scale</h3> <p>Outlier criteria: magnitude is at least 6.0, affects at least 25% of layers, and affects at least 6% of the sequence dimensions.</p> <p>These outliers affect the performance of the model significantly. For instance, when authors remove 7 random feature dimensions the top-1 probability decreases only 0.02-0.3%. However, when they remove 7 outlier feature dimensions the top-1 probability decreases 20-30%.</p> <h3 id="related-work">Related Work</h3> <p>nuQmm (Park et al., 2022) and ZeroQuant (Yao et al., 2022) are the most related works. They do higher quantization precision but require custom CUDA kernel development. Both methods evaluate 2.7B and 20B parameter models.</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee, D. (2022). nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557.</span>

<span class="c">Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and He, Y. (2022). Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861.</span>
</code></pre></div></div> <p>GLM-130B (Zeng et al., 2022), does zero-degradation 8-bit quantization, they perform full 16-bit precision matrix multiplication with 8-bit weight storage.</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., et al. (2022). Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</span>
</code></pre></div></div> <h1 id="third-pass">Third Pass</h1> <p>The papers comments on outlier features was insightful. Maybe these features could be further analysed (or already has been in the current research).</p> <p>If these features are understood better, they could be used for enhanced interpretability frameworks for LLMs. Moreover, the paper’s way of treating these features with higher precision is an interesting approach, in future research this should be remembered. It could be useful in other efficiency works as well.</p> <h4 id="references">References</h4> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">dettmers2022gpt3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{35}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{30318--30332}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>

<span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ilharco2020high</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{High performance natural language processing}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Ilharco, Gabriel and Ilharco, Cesar and Turc, Iulia and Dettmers, Tim and Ferreira, Felipe and Lee, Kenton}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{24--27}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2020}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="lit-notes"/><category term="quantization"/><category term="transformers"/><category term="nlp"/><summary type="html"><![CDATA[My notes on the paper by Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. The paper was presented at NeurIPS 2022. The paper can be found here.]]></summary></entry><entry><title type="html">Notes on the paper - What does BERT look at? An Analysis of BERT’s Attention (ACL 2019)</title><link href="https://emirhanboge.github.io/posts/2024/11/06/what-does-bert-look-at/" rel="alternate" type="text/html" title="Notes on the paper - What does BERT look at? An Analysis of BERT’s Attention (ACL 2019)"/><published>2024-11-06T00:00:00+00:00</published><updated>2024-11-06T00:00:00+00:00</updated><id>https://emirhanboge.github.io/posts/2024/11/06/what-does-bert-look-at</id><content type="html" xml:base="https://emirhanboge.github.io/posts/2024/11/06/what-does-bert-look-at/"><![CDATA[<p>My notes on the paper by Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. The paper was presented at ACL 2019. The paper can be found <a href="https://arxiv.org/abs/1906.04341">here</a>.</p> <p>I do 3 passes of reading a paper. The first pass is to get a general idea of what the paper is about. The second pass is to understand the details of the paper. The third pass is to understand the paper in depth.</p> <p>For this particular paper I did only the first pass because I wanted to understand the general idea of the paper.</p> <h1 id="first-pass">First Pass</h1> <ol> <li><strong>What is the problem?</strong> The paper investigates what BERT attends to when making predictions.</li> <li><strong>Why is it important?</strong> Understanding what BERT or any other language model attends to is important as it can help us understand how these models <strong>reason</strong>.</li> </ol> <h3 id="abstract">Abstract</h3> <p>Previous work focused on:</p> <ul> <li>Model outputs (final layer of the model)</li> <li>Internal vector representations (this could be the hidden states of the model)</li> </ul> <p>BERT’s attention heads focused on:</p> <ul> <li>Delimeter tokens (e.g. [CLS], [SEP])</li> <li>Some positional offsets, meaning that the attention heads are not just focusing on the token itself but also on the tokens around it.</li> <li>Or broadly on the entire sentence.</li> </ul> <p>Some attention heads reflected already known linguistic phenomena:</p> <ul> <li>e.g. attention heads that focused on objects of verbs, coreference, etc.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>The paper proposed different methods to understand attention.</p> <p>They state that in the attention maps linguistic knowledge can be found. They do something called <strong>probing attention maps</strong> to understand what the attention heads are focusing on.</p> <p>So I believe the paper shown that BERT reflects linguistic phenomena in its attention heads. This is a very revealing insight into how BERT works.</p> <h3 id="introduction">Introduction</h3> <p>Heads learn different types of information, in this case linguistic information. Language models like BERT by training on unlabeled data inherently learns linguistic information, thus there is no need for explicit linguistic features.</p> <h4 id="references">References</h4> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">clark2019does</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{What Does Bert Look At? An Analysis of Bert’s Attention}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Clark, Kevin}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:1906.04341}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2019}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="lit-notes"/><category term="bert"/><category term="attention"/><category term="interpretability"/><summary type="html"><![CDATA[My notes on the paper by Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. The paper was presented at ACL 2019. The paper can be found here.]]></summary></entry></feed>
---
layout: post
title: Notes on the paper - LLM.int8() 8-bit Matrix Multiplication for Transformers at Scale (NeurIPS 2022)
date: 2024-11-07
description:
tags: [quantization, transformers, nlp]
categories: [research-notes]
featured: true
---

My notes on the paper by Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. The paper was presented at NeurIPS 2022. The paper can be found [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf).

# First Pass

1. **What is the problem?** The paper investigates how to quantize the attention mechanism in transformers to 8-bit precision.
2. **Why is it important?** Quantizing the attention mechanism in transformers to 8-bit precision can reduce the memory footprint and computational cost of transformers.

### Abstract
An llm quantization method with in8 matrix multiplication. Memory required for inference is reduced by half. The most interesting part for me is that the authors state that there are **emergent features** in transformers. They use vector-wise quantization to quantize the attention mechanism in transformers. Most importantly, for the emergent features they use a different way of decomposing the attention mechanism.

### Introduction
The feed-forward and attention layers with their matrix multiplication operations make up the 95% of the parameters and 65-85% of the computation for large transformer language models (For LLMs with >= 6.7B parameters) (Ilharco et al., 2020).

First multi-billion scale Int8 quantization procedure.

- 32-bit weights to 8-bit weights

With vector-wise quantization the method retains performance up to 2.7B parameters. Beyond 6.7B parameters emergence of extreme outliers in the feature dimensions during inference, making up 0.1% of all input features. Zeroing these features out decreases top-1 attention softmax probability mass by more than 20%, and affects validation perplexity by 600-1000%.

To compensate for these outliers the paper performs 16-bit matrix multiplication on the outliers, and vector-wise quantization on the rest of the features (99.9%).

### Broader Impacts
Making LLMs more accessible for future research. This could lead to both positive and negative impacts.

# Second Pass

### Background
High-precision asymmetric quantization (zeropoint quantization): high precision by using the full bit-range but is rarely used in practice due to the high memory and computational cost. Scaling with the normalized dynamic range and then shifting by the zeropoint.

Symmetric quantization (absolute maximum quantization): the most commonly used technique. Dividing by the infinity norm and multiplying by 127. Values in [-127, 0) are not used.

### Int8 Matrix Multiplicatiobn at Scale (Method of the paper)

Main challenge: quantization methods use a single scaling constant per tensor, an outlier could reduce precision.

#### Vector-wise Quantization
View matrix multiplication as a sequence of independent inner products.

Hidden States: \( X_{f16} \in \mathbb{R}^{b \times h} \)
Weight Matrix: \( W_{f16} \in \mathbb{R}^{h \times o} \)

Assign scaling constant \( c_{x_{f16}} \) to each row of \( X_{f16} \) and \( c_w \) to each row of \( W_{f16} \).

Dequantize by: \( \frac{1}{c_{x_{f16}}c_{w{f16}}}\).


#### References
```bibtex
@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@inproceedings{ilharco2020high,
  title={High performance natural language processing},
  author={Ilharco, Gabriel and Ilharco, Cesar and Turc, Iulia and Dettmers, Tim and Ferreira, Felipe and Lee, Kenton},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts},
  pages={24--27},
  year={2020}
}
```
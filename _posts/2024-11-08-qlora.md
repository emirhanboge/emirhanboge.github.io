---
layout: post
title: Notes on the paper - LoRA:Low-Rank Adaptation of Large Language Models (ICLR 2022)
permalink: /posts/2024/11/08/qlora/
date: 2024-11-08
description:
tags: [parameter-efficient, transformers, nlp, llm]
categories: [lit-notes]
featured: true
---

My notes on the paper by Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. The paper was presented at ICLR 2022. The paper can be found [here](https://arxiv.org/pdf/2106.09685).

# First Pass

1. **What is the problem?** The paper investigates how to adapt large language models to new tasks with fewer parameters.
2. **Why is it important?** Adapting large language models to new tasks with fewer parameters can reduce the computational cost and memory footprint of these models.

### Abstract
LLMs fine-tuning to new tasks is computationall expensive as they have lots of parameters, and all of them are trainable. The paper proposes a method that freezes all the original parameters and **injects** new trainable rank decomposition matrices.
The parameters are reduced 10.000x compared to the original model. The method is called LoRA.

### Introduction
The previous methods had a problem in inference latency, which is the time it takes to make a prediction, and they reduce the model's usable sequence length, which is the length of the input sequence the model can handle. Moreover, they also does not match the baselines. The paper proposes a method that does not have these problems.

Li et al. (2018); Aghajanyan et al. (2020) => learned over-parametrized models depend on low intrinsic dimension.

```bibtex
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes. arXiv:1804.08838 [cs, stat], April 2018a. URL http: //arxiv.org/abs/1804.08838. arXiv: 1804.08838.

Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], December 2020. URL http://arxiv.org/abs/2012.13255.
```

- We can use many matrices for many tasks, freeze one and switch to the other matrix to work on another task.
- Only injected smaller low-rank matrices are optimized.


references

#### References
```bibtex
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
```
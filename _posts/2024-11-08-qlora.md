---
layout: post
title: Notes on the paper - LoRA:Low-Rank Adaptation of Large Language Models (ICLR 2022)
permalink: /posts/2024/11/08/qlora/
date: 2024-11-08
description:
tags: [parameter-efficient, transformers, nlp, llm]
categories: [lit-notes]
featured: true
---

My notes on the paper by Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. The paper was presented at ICLR 2022. The paper can be found [here](https://arxiv.org/pdf/2106.09685).

# First Pass

1. **What is the problem?** The paper investigates how to adapt large language models to new tasks with fewer parameters.
2. **Why is it important?** Adapting large language models to new tasks with fewer parameters can reduce the computational cost and memory footprint of these models.

LLMs fine-tuning to new tasks is computationall expensive as they have lots of parameters, and all of them are trainable. The paper proposes a method that freezes all the original parameters and **injects** new trainable rank decomposition matrices.
The parameters are reduced 10.000x compared to the original model. The method is called LoRA.

The previous methods had a problem in inference latency, which is the time it takes to make a prediction, and they reduce the model's usable sequence length, which is the length of the input sequence the model can handle. Moreover, they also does not match the baselines. The paper proposes a method that does not have these problems.

Li et al. (2018); Aghajanyan et al. (2020) => learned over-parametrized models depend on low intrinsic dimension.

```bibtex
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes. arXiv:1804.08838 [cs, stat], April 2018a. URL http: //arxiv.org/abs/1804.08838. arXiv: 1804.08838.

Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], December 2020. URL http://arxiv.org/abs/2012.13255.
```

- We can use many matrices for many tasks, freeze one and switch to the other matrix to work on another task.
- Only injected smaller low-rank matrices are optimized.

# Second Pass
For each different task traditionally we learn a different set of parameters, so having many independent fine-tuned models is not feasible. Lora hence is a suitable method for adapting to many new tasks with fewer parameters.

Aghajanyan et al. (2020) => random projection to a smaller subspace can still work.

LoRA => Weights also have a low intrinsic rank.

Original weights are frozen, matrices A and B are injected and optimized.

$$W = W_0 + B \cdot A$$

A => Random Gaussian initialization

B => Zero initialization

LoRA generalizes to original fine-tuning when the number of parameters increases to the number of original parameters.

Note: in this study only the attention weights are adapted, no MLPs.

- They compared it to BitFit: only bias vectors are trained, rest is frozen. (Zaken et al., 2021)

```bibtex
Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models, 2021.
```

# Third Pass
- Can we represent A and B in a more compact way?
- Can we use a different initialization for A and B?
- Can we use a different optimization strategy for A and B? (e.g. Adam for A and SGD for B). This is because B is initialized to zero and Adam might not be able to update it properly.

#### References
```bibtex
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
```
<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Notes on the paper - LoRA vs Full Fine-Tuning:An Illusion of Equivalence | Emirhan Böge </title> <meta name="author" content="Emirhan Böge"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="/assets/libs/mdb/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="/assets/libs/google_fonts/google-fonts.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%87&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emirhanboge.github.io/posts/2024/11/13/lora-vs-fullft/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Emirhan Böge </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/posts/">posts <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Notes on the paper - LoRA vs Full Fine-Tuning:An Illusion of Equivalence</h1> <p class="post-meta"> Created in November 13, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> lora   <i class="fa-solid fa-hashtag fa-sm"></i> llm   <i class="fa-solid fa-hashtag fa-sm"></i> interpretability   ·   <i class="fa-solid fa-tag fa-sm"></i> lit-notes </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My notes on the paper by Reece Shuttleworth, Jacob Andreas, Antonio Torralba, and Pratyusha Sharma. The paper can be found <a href="https://arxiv.org/pdf/2410.21228v1" rel="external nofollow noopener" target="_blank">here</a>.</p> <h1 id="first-pass">First Pass</h1> <p>Are Full Fine-Tuning and LoRA equivalent?</p> <ul> <li> <p><strong>Full Fine-Tuning</strong> =&gt; Fine-tune the entire model on a new task.</p> </li> <li> <p><strong>LoRA</strong> =&gt; Fine-tune only the adapter on a new task.</p> </li> </ul> <p>These have different structure considering their singular value decomposition (SVD) of the weight matrix. The paper shows that the two methods are not equivalent.</p> <p>Fine-tuned models generalize outside the adaptation task’s distribution but LoRA might have some problems with this. LoRA has <strong>intruder dimensions</strong>, as called by the authors, that are not seen in the full fine-tuning. LoRA with intruder dimensions might achieve comparable performance to full fine-tuning but when used with multiple tasks the performance drops.</p> <p><strong>LoRA and full fine-tuning use different subspaces of the weight matrix (intruder dimensions).</strong></p> <p><strong>LoRA with intruder dimensions forgets pretraining distribution.</strong></p> <p>LoRA and full fiine-tuning have differences in neuron’s angle and magnitude (Liu et al., 2024).</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, KwangTing Cheng, and Min-Hung Chen. DoRA: Weight-Decomposed Low-Rank Adaptation. In Proceedings of the 41st International Conference on Machine Learning. International Conference on Machine Learning, 2024. URL https://arxiv.org/abs/2402.09353.</span>
</code></pre></div></div> <p>LoRA cannot match full fine-tuning in code generation, which is considered as a hard task (Biderman et al., 2024; Zhuo et al., 2024). (Note: Biderman et al. (2024) also showed that setting a = 2r in LoRA improves results for higher ranks.)</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John P. Cunningham. LoRA Learns Less and Forgets Less. Transactions on Machine Learning Research, 2024. URL https://arxiv.org/abs/2405.09673.</span>

<span class="c">Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, and Niklas Muennighoff. Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models, 2024. URL https://arxiv.org/abs/2401.00788.</span>
</code></pre></div></div> <p>It also has problems with long-form text generation (Ivison et al., 2023)</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2, 2023. URL https://arxiv. org/abs/2311.10702.</span>
</code></pre></div></div> <p>This paper aims to understand if these downsides of LoRA indicate a fundamental limitation of LoRA or if they can be addressed by better understanding the method.</p> <h1 id="second-pass">Second Pass</h1> <p>First Aghajanyan et al. (2021) proposed the idea that LLMs has low instrinsic rank, so significantly less number of parameters are needed to have similar performance to full fine-tuning. This is the basis of LoRA.</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, August 2021. URL https://aclanthology.org/2021.acl-long.568.</span>
</code></pre></div></div> <p>Sharma et al. (2024) showed that SVD of neuron’s weight matrix can be used to understand fine-tuning’s effect on pretrained weights.</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Pratyusha Sharma, Jordan T. Ash, and Dipendra Misra. The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= ozX92bu8VA.</span>
</code></pre></div></div> <p>This paper uses this idea to see if LoRA and full fine-tuning’s singular vectors in weight matrices are similar using to pretrained weights. They found out LoRA is not similar to pretrained singular vectors as full fine-tuning. Interestingly, with LoRA there are some singular vectors, <strong>(intruder dimensions)</strong>, that have very low cosine similarity to any pretrained singular vector.</p> <p><strong>LoRA and full fine-tuning have differences in the changes they make to the pretrained weights.</strong></p> <p>Authors find out that LoRA consistently contains intruder dimensions (rank &lt;= 16, the number decreases when rank gets increased as LoRA starts to resemble full fine-tuning), and full fine-tuning almost never does. Hence, it can be stated that full fine-tuning is more similar to the pretrained model than LoRA.</p> <p><strong>More fine-tuning data leads to more intruder dimensions in LoRA.</strong></p> <hr> <p>LoRA has intruder dimensions that affect pretrained weights’ impact. This is detrimental when using LoRA on tasks outside of the fine-tuning task’s distribution.</p> <hr> <h1 id="third-pass">Third Pass</h1> <ul> <li>Can we modify LoRA to prevent the formation of intruder dimensions while maintaining its parameter efficiency?</li> <li>How do intruder dimensions interact when multiple LoRA adapters are combined?</li> </ul> <h3 id="references">References</h3> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">shuttleworth2024lora</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{LoRA vs Full Fine-tuning: An Illusion of Equivalence}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Shuttleworth, Reece and Andreas, Jacob and Torralba, Antonio and Sharma, Pratyusha}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2410.21228}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/11/13/towards-modular-llms/">Notes on the paper - Towards Modular LLMs by Building and Reusing a Library of LoRAs</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/11/10/lora/">Notes on the paper - LoRA:Low-Rank Adaptation of Large Language Models (ICLR 2022)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/11/07/llm-8int/">Notes on the paper - LLM.int8() 8-bit Matrix Multiplication for Transformers at Scale (NeurIPS 2022)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/11/06/what-does-bert-look-at/">Notes on the paper - What does BERT look at? An Analysis of BERT’s Attention (ACL 2019)</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Emirhan Böge. Last updated: November 13, 2024. </div> </footer> <script src="/assets/libs/jquery/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="/assets/libs/mdb/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="/assets/libs/masonry/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="/assets/libs/imagesloaded/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="/assets/libs/medium_zoom/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script>MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,packages:["base","ams","noerrors","physics"]},loader:{load:["[tex]/ams","[tex]/noerrors","[tex]/physics"]},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],processHtmlClass:"tex2jax_process"}};</script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" type="text/javascript"> </script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-posts",title:"posts",description:"",section:"Navigation",handler:()=>{window.location.href="/posts/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-notes-on-the-paper-towards-modular-llms-by-building-and-reusing-a-library-of-loras",title:"Notes on the paper - Towards Modular LLMs by Building and Reusing a...",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/13/towards-modular-llms/"}},{id:"post-notes-on-the-paper-lora-vs-full-fine-tuning-an-illusion-of-equivalence",title:"Notes on the paper - LoRA vs Full Fine-Tuning:An Illusion of Equivalence",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/13/lora-vs-fullft/"}},{id:"post-notes-on-the-paper-lora-low-rank-adaptation-of-large-language-models-iclr-2022",title:"Notes on the paper - LoRA:Low-Rank Adaptation of Large Language Models (ICLR 2022)...",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/10/lora/"}},{id:"post-notes-on-the-paper-llm-int8-8-bit-matrix-multiplication-for-transformers-at-scale-neurips-2022",title:"Notes on the paper - LLM.int8() 8-bit Matrix Multiplication for Transformers at Scale...",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/07/llm-8int/"}},{id:"post-notes-on-the-paper-what-does-bert-look-at-an-analysis-of-bert-s-attention-acl-2019",title:"Notes on the paper - What does BERT look at? An Analysis of...",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/06/what-does-bert-look-at/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=b9uTdCAAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/emirhanboge","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/emirhanb","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>
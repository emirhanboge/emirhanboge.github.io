<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Notes on the paper - LLM.int8() 8-bit Matrix Multiplication for Transformers at Scale (NeurIPS 2022) | Emirhan Böge </title> <meta name="author" content="Emirhan Böge"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="/assets/libs/mdb/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="/assets/libs/google_fonts/google-fonts.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%87&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emirhanboge.github.io/posts/2024/11/07/llm-8int/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Emirhan Böge </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/posts/">posts <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Notes on the paper - LLM.int8() 8-bit Matrix Multiplication for Transformers at Scale (NeurIPS 2022)</h1> <p class="post-meta"> Created in November 07, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> quantization   <i class="fa-solid fa-hashtag fa-sm"></i> transformers   <i class="fa-solid fa-hashtag fa-sm"></i> nlp   ·   <i class="fa-solid fa-tag fa-sm"></i> lit-notes </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My notes on the paper by Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. The paper was presented at NeurIPS 2022. The paper can be found <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf" rel="external nofollow noopener" target="_blank">here</a>.</p> <h1 id="first-pass">First Pass</h1> <ol> <li> <strong>What is the problem?</strong> The paper investigates how to quantize the attention mechanism in transformers to 8-bit precision.</li> <li> <strong>Why is it important?</strong> Quantizing the attention mechanism in transformers to 8-bit precision can reduce the memory footprint and computational cost of transformers.</li> </ol> <h3 id="abstract">Abstract</h3> <p>An llm quantization method with in8 matrix multiplication. Memory required for inference is reduced by half. The most interesting part for me is that the authors state that there are <strong>emergent features</strong> in transformers. They use vector-wise quantization to quantize the attention mechanism in transformers. Most importantly, for the emergent features they use a different way of decomposing the attention mechanism.</p> <h3 id="introduction">Introduction</h3> <p>The feed-forward and attention layers with their matrix multiplication operations make up the 95% of the parameters and 65-85% of the computation for large transformer language models (For LLMs with &gt;= 6.7B parameters) (Ilharco et al., 2020).</p> <p>First multi-billion scale Int8 quantization procedure.</p> <ul> <li>32-bit weights to 8-bit weights</li> </ul> <p>With vector-wise quantization the method retains performance up to 2.7B parameters. Beyond 6.7B parameters emergence of extreme outliers in the feature dimensions during inference, making up 0.1% of all input features. Zeroing these features out decreases top-1 attention softmax probability mass by more than 20%, and affects validation perplexity by 600-1000%.</p> <p>To compensate for these outliers the paper performs 16-bit matrix multiplication on the outliers, and vector-wise quantization on the rest of the features (99.9%).</p> <h3 id="broader-impacts">Broader Impacts</h3> <p>Making LLMs more accessible for future research. This could lead to both positive and negative impacts.</p> <h1 id="second-pass">Second Pass</h1> <h3 id="background">Background</h3> <p>High-precision asymmetric quantization (zeropoint quantization): high precision by using the full bit-range but is rarely used in practice due to the high memory and computational cost. Scaling with the normalized dynamic range and then shifting by the zeropoint.</p> <p>Symmetric quantization (absolute maximum quantization): the most commonly used technique. Dividing by the infinity norm and multiplying by 127. Values in [-127, 0) are not used.</p> <h3 id="int8-matrix-multiplicatiobn-at-scale-method-of-the-paper">Int8 Matrix Multiplicatiobn at Scale (Method of the paper)</h3> <p>Main challenge: quantization methods use a single scaling constant per tensor, an outlier could reduce precision.</p> <h4 id="vector-wise-quantization">Vector-wise Quantization</h4> <p>View matrix multiplication as a sequence of independent inner products.</p> <p>Hidden States: $X_{f16} \in \mathbb{R}^{b \times h}$</p> <p>Weight Matrix: $W_{f16} \in \mathbb{R}^{h \times o}$</p> <p>Assign scaling constant $c_{x_{f16}}$ to each row of $X_{f16}$ and $c_w$ to each row of $W_{f16}$.</p> <p>Dequantize by: $\frac{1}{c_{x_{f16}}c_{w_{f16}}}$</p> <h4 id="mixed-precision-decomposition-for-emergent-features">Mixed-precision Decomposition (for emergent features)</h4> <p>For the emergent features, the authors use a different way of decomposing the attention mechanism. These are important for transformer performance, and thus they need higher precision. As they are scarce in the input, they can be processed with higher precision and without a significant increase in memory or computational cost.</p> <p>Outliers should have a magnitude larget than the threshold $\alpha$. This paper uses $\alpha = 6.0$.</p> <p>For evaluation:</p> <ul> <li>Perplexity (authors say this is a robust measure sensitive to quantization)</li> <li>Zeroshot accuracy degradation</li> </ul> <p><strong>Results</strong>: absmax, row-wise, and zeropoint quantization methods fail as the parameters scale up, wheras LLM.int8() retains performance. It runs two times faster for large matrix multiplications (up to 175B parameters).</p> <h3 id="emergent-large-magnitude-features-in-transformers-at-scale">Emergent Large Magnitude Features in Transformers at Scale</h3> <p>Outlier criteria: magnitude is at least 6.0, affects at least 25% of layers, and affects at least 6% of the sequence dimensions.</p> <p>These outliers affect the performance of the model significantly. For instance, when authors remove 7 random feature dimensions the top-1 probability decreases only 0.02-0.3%. However, when they remove 7 outlier feature dimensions the top-1 probability decreases 20-30%.</p> <h3 id="related-work">Related Work</h3> <p>nuQmm (Park et al., 2022) and ZeroQuant (Yao et al., 2022) are the most related works. They do higher quantization precision but require custom CUDA kernel development. Both methods evaluate 2.7B and 20B parameter models.</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee, D. (2022). nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557.</span>

<span class="c">Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and He, Y. (2022). Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861.</span>
</code></pre></div></div> <p>GLM-130B (Zeng et al., 2022), does zero-degradation 8-bit quantization, they perform full 16-bit precision matrix multiplication with 8-bit weight storage.</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., et al. (2022). Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</span>
</code></pre></div></div> <h1 id="third-pass">Third Pass</h1> <p>The papers comments on outlier features was insightful. Maybe these features could be further analysed (or already has been in the current research).</p> <p>If these features are understood better, they could be used for enhanced interpretability frameworks for LLMs. Moreover, the paper’s way of treating these features with higher precision is an interesting approach, in future research this should be remembered. It could be useful in other efficiency works as well.</p> <h4 id="references">References</h4> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">dettmers2022gpt3</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{35}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{30318--30332}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>

<span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ilharco2020high</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{High performance natural language processing}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Ilharco, Gabriel and Ilharco, Cesar and Turc, Iulia and Dettmers, Tim and Ferreira, Felipe and Lee, Kenton}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{24--27}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2020}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/11/13/towards-modular-llms/">Notes on the paper - Towards Modular LLMs by Building and Reusing a Library of LoRAs</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/11/13/lora-vs-fullft/">Notes on the paper - LoRA vs Full Fine-Tuning:An Illusion of Equivalence</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/11/10/lora/">Notes on the paper - LoRA:Low-Rank Adaptation of Large Language Models (ICLR 2022)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/11/06/what-does-bert-look-at/">Notes on the paper - What does BERT look at? An Analysis of BERT’s Attention (ACL 2019)</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Emirhan Böge. Last updated: November 13, 2024. </div> </footer> <script src="/assets/libs/jquery/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="/assets/libs/mdb/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="/assets/libs/masonry/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="/assets/libs/imagesloaded/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="/assets/libs/medium_zoom/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script>MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,packages:["base","ams","noerrors","physics"]},loader:{load:["[tex]/ams","[tex]/noerrors","[tex]/physics"]},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],processHtmlClass:"tex2jax_process"}};</script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" type="text/javascript"> </script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-posts",title:"posts",description:"",section:"Navigation",handler:()=>{window.location.href="/posts/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-notes-on-the-paper-towards-modular-llms-by-building-and-reusing-a-library-of-loras",title:"Notes on the paper - Towards Modular LLMs by Building and Reusing a...",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/13/towards-modular-llms/"}},{id:"post-notes-on-the-paper-lora-vs-full-fine-tuning-an-illusion-of-equivalence",title:"Notes on the paper - LoRA vs Full Fine-Tuning:An Illusion of Equivalence",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/13/lora-vs-fullft/"}},{id:"post-notes-on-the-paper-lora-low-rank-adaptation-of-large-language-models-iclr-2022",title:"Notes on the paper - LoRA:Low-Rank Adaptation of Large Language Models (ICLR 2022)...",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/10/lora/"}},{id:"post-notes-on-the-paper-llm-int8-8-bit-matrix-multiplication-for-transformers-at-scale-neurips-2022",title:"Notes on the paper - LLM.int8() 8-bit Matrix Multiplication for Transformers at Scale...",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/07/llm-8int/"}},{id:"post-notes-on-the-paper-what-does-bert-look-at-an-analysis-of-bert-s-attention-acl-2019",title:"Notes on the paper - What does BERT look at? An Analysis of...",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/06/what-does-bert-look-at/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=b9uTdCAAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/emirhanboge","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/emirhanb","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>My notes on the paper by Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. The paper was presented at ICLR 2022. The paper can be found <a href="https://arxiv.org/pdf/2106.09685" rel="external nofollow noopener" target="_blank">here</a>.</p> <h1 id="first-pass">First Pass</h1> <ol> <li> <strong>What is the problem?</strong> The paper investigates how to adapt large language models to new tasks with fewer parameters.</li> <li> <strong>Why is it important?</strong> Adapting large language models to new tasks with fewer parameters can reduce the computational cost and memory footprint of these models.</li> </ol> <h3 id="abstract">Abstract</h3> <p>LLMs fine-tuning to new tasks is computationall expensive as they have lots of parameters, and all of them are trainable. The paper proposes a method that freezes all the original parameters and <strong>injects</strong> new trainable rank decomposition matrices. The parameters are reduced 10.000x compared to the original model. The method is called LoRA.</p> <h3 id="introduction">Introduction</h3> <p>The previous methods had a problem in inference latency, which is the time it takes to make a prediction, and they reduce the modelâ€™s usable sequence length, which is the length of the input sequence the model can handle. Moreover, they also does not match the baselines. The paper proposes a method that does not have these problems.</p> <p>Li et al. (2018); Aghajanyan et al. (2020) =&gt; learned over-parametrized models depend on low intrinsic dimension.</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes. arXiv:1804.08838 [cs, stat], April 2018a. URL http: //arxiv.org/abs/1804.08838. arXiv: 1804.08838.</span>

<span class="c">Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], December 2020. URL http://arxiv.org/abs/2012.13255.</span>
</code></pre></div></div> <ul> <li>We can use many matrices for many tasks, freeze one and switch to the other matrix to work on another task.</li> <li>Only injected smaller low-rank matrices are optimized.</li> </ul> <p>references</p> <h4 id="references">References</h4> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">hu2021lora</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Lora: Low-rank adaptation of large language models}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2106.09685}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2021}</span>
<span class="p">}</span>
</code></pre></div></div> </body></html>
<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Notes on the paper - LoRA:Low-Rank Adaptation of Large Language Models (ICLR 2022) | Emirhan Böge </title> <meta name="author" content="Emirhan Böge"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="/assets/libs/mdb/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="/assets/libs/google_fonts/google-fonts.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%87&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emirhanboge.github.io/posts/2024/11/10/lora/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Emirhan Böge </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/posts/">posts <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Notes on the paper - LoRA:Low-Rank Adaptation of Large Language Models (ICLR 2022)</h1> <p class="post-meta"> Created in November 10, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> parameter-efficient   <i class="fa-solid fa-hashtag fa-sm"></i> transformers   <i class="fa-solid fa-hashtag fa-sm"></i> nlp   <i class="fa-solid fa-hashtag fa-sm"></i> llm   ·   <i class="fa-solid fa-tag fa-sm"></i> lit-notes </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My notes on the paper by Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. The paper was presented at ICLR 2022. The paper can be found <a href="https://arxiv.org/pdf/2106.09685" rel="external nofollow noopener" target="_blank">here</a>.</p> <h1 id="first-pass">First Pass</h1> <ol> <li> <strong>What is the problem?</strong> The paper investigates how to adapt large language models to new tasks with fewer parameters.</li> <li> <strong>Why is it important?</strong> Adapting large language models to new tasks with fewer parameters can reduce the computational cost and memory footprint of these models.</li> </ol> <p>LLMs fine-tuning to new tasks is computationall expensive as they have lots of parameters, and all of them are trainable. The paper proposes a method that freezes all the original parameters and <strong>injects</strong> new trainable rank decomposition matrices. The parameters are reduced 10.000x compared to the original model. The method is called LoRA.</p> <p>The previous methods had a problem in inference latency, which is the time it takes to make a prediction, and they reduce the model’s usable sequence length, which is the length of the input sequence the model can handle. Moreover, they also does not match the baselines. The paper proposes a method that does not have these problems.</p> <p>Li et al. (2018); Aghajanyan et al. (2020) =&gt; learned over-parametrized models depend on low intrinsic dimension.</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes. arXiv:1804.08838 [cs, stat], April 2018a. URL http: //arxiv.org/abs/1804.08838. arXiv: 1804.08838.</span>

<span class="c">Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], December 2020. URL http://arxiv.org/abs/2012.13255.</span>
</code></pre></div></div> <ul> <li>We can use many matrices for many tasks, freeze one and switch to the other matrix to work on another task.</li> <li>Only injected smaller low-rank matrices are optimized.</li> </ul> <h1 id="second-pass">Second Pass</h1> <p>For each different task traditionally we learn a different set of parameters, so having many independent fine-tuned models is not feasible. Lora hence is a suitable method for adapting to many new tasks with fewer parameters.</p> <p>Aghajanyan et al. (2020) =&gt; random projection to a smaller subspace can still work.</p> <p>LoRA =&gt; Weights also have a low intrinsic rank.</p> <p>Original weights are frozen, matrices A and B are injected and optimized.</p> \[W = W_0 + B \cdot A\] <p>A =&gt; Random Gaussian initialization</p> <p>B =&gt; Zero initialization</p> <p>LoRA generalizes to original fine-tuning when the number of parameters increases to the number of original parameters.</p> <p>Note: in this study only the attention weights are adapted, no MLPs.</p> <ul> <li>They compared it to BitFit: only bias vectors are trained, rest is frozen. (Zaken et al., 2021)</li> </ul> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models, 2021.</span>
</code></pre></div></div> <h1 id="third-pass">Third Pass</h1> <ul> <li>Can we represent A and B in a more compact way?</li> <li>Can we use a different initialization for A and B?</li> <li>Can we use a different optimization strategy for A and B? (e.g. Adam for A and SGD for B). This is because B is initialized to zero and Adam might not be able to update it properly.</li> </ul> <h4 id="references">References</h4> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">hu2021lora</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Lora: Low-rank adaptation of large language models}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2106.09685}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2021}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/11/13/towards-modular-llms/">Notes on the paper - Towards Modular LLMs by Building and Reusing a Library of LoRAs</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/11/13/lora-vs-fullft/">Notes on the paper - LoRA vs Full Fine-Tuning:An Illusion of Equivalence</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/11/07/llm-8int/">Notes on the paper - LLM.int8() 8-bit Matrix Multiplication for Transformers at Scale (NeurIPS 2022)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/11/06/what-does-bert-look-at/">Notes on the paper - What does BERT look at? An Analysis of BERT’s Attention (ACL 2019)</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Emirhan Böge. Last updated: November 13, 2024. </div> </footer> <script src="/assets/libs/jquery/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="/assets/libs/mdb/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="/assets/libs/masonry/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="/assets/libs/imagesloaded/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="/assets/libs/medium_zoom/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script>MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0,packages:["base","ams","noerrors","physics"]},loader:{load:["[tex]/ams","[tex]/noerrors","[tex]/physics"]},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],processHtmlClass:"tex2jax_process"}};</script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" type="text/javascript"> </script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-posts",title:"posts",description:"",section:"Navigation",handler:()=>{window.location.href="/posts/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"post-notes-on-the-paper-towards-modular-llms-by-building-and-reusing-a-library-of-loras",title:"Notes on the paper - Towards Modular LLMs by Building and Reusing a...",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/13/towards-modular-llms/"}},{id:"post-notes-on-the-paper-lora-vs-full-fine-tuning-an-illusion-of-equivalence",title:"Notes on the paper - LoRA vs Full Fine-Tuning:An Illusion of Equivalence",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/13/lora-vs-fullft/"}},{id:"post-notes-on-the-paper-lora-low-rank-adaptation-of-large-language-models-iclr-2022",title:"Notes on the paper - LoRA:Low-Rank Adaptation of Large Language Models (ICLR 2022)...",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/10/lora/"}},{id:"post-notes-on-the-paper-llm-int8-8-bit-matrix-multiplication-for-transformers-at-scale-neurips-2022",title:"Notes on the paper - LLM.int8() 8-bit Matrix Multiplication for Transformers at Scale...",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/07/llm-8int/"}},{id:"post-notes-on-the-paper-what-does-bert-look-at-an-analysis-of-bert-s-attention-acl-2019",title:"Notes on the paper - What does BERT look at? An Analysis of...",description:"",section:"Posts",handler:()=>{window.location.href="/posts/2024/11/06/what-does-bert-look-at/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=b9uTdCAAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/emirhanboge","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/emirhanb","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>